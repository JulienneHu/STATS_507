{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c392dfac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ps10.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96f043",
   "metadata": {},
   "source": [
    "# Problem Set 10\n",
    "## Logistic regression, automatic differentiation, and neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b5d04",
   "metadata": {},
   "source": [
    "In this problem set you will study binary classification, logistic regression, and implement a simple neural network with one hidden layer.\n",
    "\n",
    "For data in this problem set, we will use the MNIST data set which we saw on problem set 6. To make things run faster, we will only use the first 1000 images in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1098f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "mnist = numpy.load('mnist.npz')\n",
    "X = mnist['images'][:1000]\n",
    "y = mnist['labels'][:1000]\n",
    "n, p = X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae14e6f",
   "metadata": {},
   "source": [
    "## Question 1: Binary classification\n",
    "\n",
    "In this exercise you will use `sklearn` to build a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ac9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c3649",
   "metadata": {},
   "source": [
    "The classifier will take as input a $28\\times 28$ grayscale MNIST image, and return `1` if the image represents the number 5, and `0` otherwise.\n",
    "\n",
    "*Note*: various algorithms implemented in `sklearn` are randomized. To utilize the same randomness as we did when generating the solutions (and hence, to ensure that your output passes the test cases), use `random_state=1` wherever necessary when calling `sklearn` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca72f48",
   "metadata": {},
   "source": [
    "**1(a)** (1 pt) Using the `mnist` data loaded above, create a standardized version of `X` where each column has zero mean and variance one. (Hint: use the `sklearn.preprocessing` module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce048973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "Xs = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "754158d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>1a</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "1a results: All test cases passed!"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"1a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37a665",
   "metadata": {},
   "source": [
    "**1(b)** (1 pt) Using the `mnist` data loaded above, create a vector `y5` which equals `1` if the the corresponding MINST image equals is of the number 5, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "928ef873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y5 = (y == 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "af6457ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>1b</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "1b results: All test cases passed!"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"1b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb72964",
   "metadata": {},
   "source": [
    "**1(c)**(1pt) Using `sklearn.model_selection.train_test_split`, divide the data into 70% training data and 30% test data. To ensure that your output matches our tests, pass the option `random_state=1` into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3505177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    Xs, y5, random_state=1, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a0889a7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>1c</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "1c results: All test cases passed!"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"1c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51e37",
   "metadata": {},
   "source": [
    "**1(d)**(2pt) Use `sklearn.linear_model.LogisticRegression` to train a binary classifier on the *training data only*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cd59447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l2\")\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d44fd382",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>1d</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "1d results: All test cases passed!"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756b3f5",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1(e)**(1pt) How accurate is your trained classifier on `X_train`/`y_train`? How accurate is it on `X_test`/`y_test`? (Use whatever measure of accuracy you think is appropriate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "62f720b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c118641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.053333333333333344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "print(zero_one_loss(clf.predict(X_train), y_train))\n",
    "print(zero_one_loss(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b4011",
   "metadata": {},
   "source": [
    "Using log-loss to check the accuracy, for X_train/y_train, it's 0 misclassification rate.  for X_test/y_test, it's about five-percent misclassification rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349b929",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1(f)**(2pt) The regularization parameter can be varied by setting `LogisticRegression(C=C)` , where `C` is the value of the regularization penalty. What happens to the test error that you computed in the previous step as you vary `C`? Can you find a setting of `C` that results in lower test error than the default value `C=1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dacd71",
   "metadata": {},
   "source": [
    "The C parameter controls the amount of regularization in the LogisticRegression object: a large value for C results in less regularization.  A high value of C tells the model to give high weight to the training data, and a lower weight to the complexity penalty. A low value tells the model to give more weight to this complexity penalty at the expense of fitting to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385ba6e",
   "metadata": {},
   "source": [
    "Based on the figure below, when C=0.1, the test error is lower than C=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "246d8908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09333333333333338, 0.046666666666666634, 0.050000000000000044, 0.053333333333333344, 0.05666666666666664, 0.05666666666666664]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "C_list = np.logspace(-3,3,num=6)\n",
    "train = []\n",
    "test = []\n",
    "for i in C_list:\n",
    "  clf = sklearn.linear_model.LogisticRegression(C=i)\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  train.append( 1-clf.score(X_train, y_train) )\n",
    "  test.append( 1-clf.score(X_test, y_test) )\n",
    "\n",
    "\n",
    "plt.semilogx(C_list, train, label='Train')\n",
    "plt.semilogx(C_list, test, label='Test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59021877",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2\n",
    "In lecture we learned that logistic regression assumes the probability model \n",
    "\n",
    "$$\\mathbb{P}(y_i=1\\mid \\mathbf{x}_i) = \\sigma(\\mathbf{x}_i^T\\boldsymbol{\\beta}),$$ \n",
    "\n",
    "where $\\mathbf{x}_i$ are rows of the data matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$, $\\mathbf{y}\\in\\{0,1\\}^n$ is a vector of binary responses, and\n",
    "\n",
    "$$\\sigma(a)=\\frac{1}{1+\\exp(-a)}$$ \n",
    "\n",
    "is a *sigmoid* function which maps  real numbers into the interval $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204fcdc",
   "metadata": {},
   "source": [
    "Logistic regression tries to find $\\hat{\\boldsymbol{\\beta}}$ that maximizes the (log)likelihood of the data. However, in complex data sets, the linear mapping $\\mathbf{x}_i\\mapsto \\mathbf{x}_i^T \\boldsymbol{\\beta}$ may not be complex enough to form a good classifier. \n",
    "\n",
    "This exercise will walk you through the process of building a more complex classifier: a one-layer feedforward neural network, known as a [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron). To implement this, we will use the machine learning library JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbf07195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75aa548",
   "metadata": {},
   "source": [
    "JAX includes a Numpy-like interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a246bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2094fa8",
   "metadata": {},
   "source": [
    "You should use the `jnp` library for all array manipulations in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63b2f4",
   "metadata": {},
   "source": [
    "**2(a)**(2pt) Define a *neuron* to be a function that takes three arguments: an *input* vector $x\\in\\mathbb{R}^p$, a *weight vector* $w \\in \\mathbb{R}^p$, and a *bias* $b\\in \\mathbb{R}$. The neuron should return `1` if $w^T x > b$ and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8cc7dc05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neuron(x, w, b):\n",
    "    return (w.dot(x)-b>0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6af235a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>2a</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "2a results: All test cases passed!"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"2a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76c4fa",
   "metadata": {},
   "source": [
    "**2(b)**(2pt) The `neuron()` function you wrote in the previous step models the behavior of an actual neuron in the brain. However, it is difficult to work with computationally because it jumps suddenly from 0 to 1 as soon as $w^Tx-b>0$. \n",
    "\n",
    "Define a second function, `smooth_neuron(x, w, b)`, which takes the same arguments and returns $\\sigma(x^T w - b)$, where $\\sigma$ is the sigmoid function defined above. In what sense is the `smooth_neuron` function an approximation to `neuron` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "73aa4549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth_neuron(x, w, b):\n",
    "    return jax.nn.sigmoid(w.dot(x)-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2fc18b0a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>2b</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "2b results: All test cases passed!"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"2b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a4dc6",
   "metadata": {},
   "source": [
    "**2(c)**(2pt) Define function `hidden_layer(x, W, b)` which accepts three parameters:\n",
    "  - An input vector $x\\in\\mathbb{R}^p$;\n",
    "  - A weight matrix $W\\in\\mathbb{R}^{K\\times p}$\n",
    "  - A bias vector $b \\in \\mathbb{R}^{K}$.\n",
    "The output of `hidden_layer(x, W, b)` should be a vector `h` of dimension `K`, whose `i`-th entry is:\n",
    "```\n",
    "h[i] = smooth_neuron(x, w[i], b[i]).\n",
    "```\n",
    "\n",
    "*Note*: don't use for loops in your implementation. Instead, use `jax.vmap` as shown in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "72b298b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4aec51ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hidden_layer(x, W, b):\n",
    "    f = vmap(smooth_neuron, in_axes=(None, 0, 0))\n",
    "    return f(x, W, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3aa358eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>2c</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "2c results: All test cases passed!"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"2c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfc174",
   "metadata": {},
   "source": [
    "**2(d)**(2pt)\n",
    "The MLP is completed by passing the output of the hidden layer through a final neuron. Mathematically, given the $h$ vector returned from the `hidden_layer` function defined above, the MLP returns \n",
    "\n",
    "$$\\text{mlp}(x) = \\sigma(h^T w_0 + b_0),$$\n",
    "\n",
    "where $w_0\\in\\mathbb{R}^K$ and $b_0\\in\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff605ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(params, x):\n",
    "    '''\n",
    "    Multi-layer perceptron.\n",
    "\n",
    "    Args:\n",
    "        params: a dict with four entries:\n",
    "            W, b: arguments to the `hidden_layer` function.\n",
    "            w0, b0: K-dimensional arguments to the smooth_neuron function\n",
    "        x: the input\n",
    "\n",
    "    Returns:\n",
    "        mlp evaluated on input x, i.e. the probability that the class label y=1 given x.\n",
    "\n",
    "    Note:\n",
    "        The number of hidden units is controlled by the dimension of W, b, w0, b0.\n",
    "    '''\n",
    "    h = hidden_layer(x, params['W'], params['b'])\n",
    "    return jax.nn.sigmoid(h.dot(params['w0'])-params['b0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0648f778",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>2d</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "2d results: All test cases passed!"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"2d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919708e1",
   "metadata": {},
   "source": [
    "**2(e)**(4pt) Define a function `mlp_loss(params, X, y)` which takes three arguments: a `params` dict, which will be passed to the `mlp` function, a matrix $X$ of features, and a vector $y\\in\\{0,1\\}$ of class labels. The function should return the negative log-likelihood of the data when they are classified using the a binary classifier based on the given MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1e3fd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import xlogy, xlog1py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20059f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp_loss(params, X, y):\n",
    "  vmlp = jax.vmap(mlp, in_axes=[None,0])\n",
    "  p = jnp.clip(vmlp(params, X), 1e-8, 1-1e-8)\n",
    "  loss = -jnp.sum(xlogy(y, p) + xlog1py(1 - y, -p))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c4ca72a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>2e</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "2e results: All test cases passed!"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"2e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef76b99",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**2(f)** How long does it take you to evaluate `mlp_loss` for the entire MNIST dataset? What about if you transform `mlp_loss` using `@jax.jit` first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f0d7489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist['images']\n",
    "y = mnist['labels']\n",
    "params = {'W' : jnp.array([[1.0], [2.0], [3.0], [4.0]]), \n",
    "                      'b' : jnp.array([13.0, -8.0, 10.5, 1.0]), \n",
    "                      'w0' : jnp.array([1.0, -1.0, 0.0, 0.0]), \n",
    "                      'b0' : 3.0}\n",
    "\n",
    "params['W'] = jnp.tile(params['W'], (1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ddc10770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 106 ms, sys: 95.1 ms, total: 201 ms\n",
      "Wall time: 95.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(1393392.5, dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mlp_loss(params, jnp.array(X), jnp.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f33af542",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def new_mlp(params,X,y):\n",
    "    return mlp_loss(params, jnp.array(X), jnp.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d08599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.1 ms, sys: 7.4 ms, total: 100 ms\n",
      "Wall time: 99.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(1393392.5, dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "new_mlp(params,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a95cf",
   "metadata": {},
   "source": [
    "With @jax.jit it is much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a83cee",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**2(g)** (For fun) Train the multi-layer perceptron such that it has better (lower) test error on the classification problem from Question 1 compared to ordinary logistic regression.\n",
    "\n",
    "*Hints*: \n",
    "- Training will be exceptionally slow unless you make use of gradients via the `@jax.grad` and/or `@jax.value_and_grad` transformations. (Be sure to jit these calls as well.)\n",
    "- You will likely need to add a regularization term to the loss function.\n",
    "- `jax.scipy.optimize.minimize` can be used to easily train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fad34a",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293e081",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0148ff4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a902c2d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1a results: All test cases passed!\n",
       "\n",
       "1b results: All test cases passed!\n",
       "\n",
       "1c results: All test cases passed!\n",
       "\n",
       "1d results:\n",
       "    1d - 1 result:\n",
       "        Trying:\n",
       "            assert sum(clf.predict(X_test)) == 27\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 1, in 1d 0\n",
       "        Failed example:\n",
       "            assert sum(clf.predict(X_test)) == 27\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/opt/homebrew/anaconda3/envs/math/lib/python3.8/doctest.py\", line 1336, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest 1d 0[0]>\", line 1, in <module>\n",
       "                assert sum(clf.predict(X_test)) == 27\n",
       "            AssertionError\n",
       "\n",
       "2a results: All test cases passed!\n",
       "\n",
       "2b results: All test cases passed!\n",
       "\n",
       "2c results: All test cases passed!\n",
       "\n",
       "2d results: All test cases passed!\n",
       "\n",
       "2e results: All test cases passed!"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49d754",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Upload this .zip file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "405f7da2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <p>Your submission has been exported. Click <a href=\"ps10_2022_04_10T17_17_34_228537.zip\" download=\"ps10_2022_04_10T17_17_34_228537.zip\" target=\"_blank\">here</a>\n",
       "            to download the zip file.</p>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557b281",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "otter": {
   "tests": {
    "1a": {
     "name": "1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> m = Xs.mean(axis = 0)\n>>> sd = Xs.std(axis = 0)\n>>> assert sum(abs(m)) < 1e-10\n>>> assert np.all(np.logical_or(abs(sd - 1) < 1e-10, abs(sd - 0) < 1e-10))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "1b": {
     "name": "1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> place = np.where(y5 == 1)[0]\n>>> assert len(place) == 92\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "1c": {
     "name": "1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert X_train.shape[0] == 700\n>>> assert y_train.shape[0] == 700\n>>> assert X_test.shape[0] == 300\n>>> assert y_test.shape[0] == 300\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "1d": {
     "name": "1d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sum(clf.predict(X_test)) == 27\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2a": {
     "name": "2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x = jnp.array([1.0,2.0,3.0])\n>>> w = jnp.array([4.0,5.0,6.0])\n>>> b1 = 33.0\n>>> b2 = 32.0\n>>> b3 = 31.0\n>>> assert neuron(x,w,b1) == 0\n>>> assert neuron(x,w,b2) == 0\n>>> assert neuron(x,w,b3) == 1\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2b": {
     "name": "2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x = jnp.array([1.0,2.0,3.0])\n>>> w = jnp.array([4.0,5.0,6.0])\n>>> b1 = 39.0\n>>> b2 = 32.0\n>>> b3 = 33.0\n>>> assert abs(smooth_neuron(x,w,b1)-0.00091105)<1e-7\n>>> assert abs(smooth_neuron(x,w,b2)-0.5) < 1e-7\n>>> assert abs(smooth_neuron(x,w,b3)-0.26894143)<1e-7\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2c": {
     "name": "2c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x = jnp.array([1.0,2.0, 3.0])\n>>> w = jnp.array([[1.0, 2.0, 3.0], [-1.0, 2.0, -4.0], [-1.0, -2.0, 5.0], [0.0,0.0,0.0]])\n>>> b = jnp.array([13.0, -8.0, 10.5, 1.0])\n>>> assert sum(abs(hidden_layer(x,w,b)-jnp.array([0.7310586, 0.26894143, 0.37754068, 0.26894143])))<1e-5\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2d": {
     "name": "2d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x = jnp.array([1.0, 2.0, 3.0])\n>>> params = {'W' : jnp.array([[1.0, 2.0, 3.0], [-1.0, 2.0, -4.0], [-1.0, -2.0, 5.0], [0.0,0.0,0.0]]), \n...               'b' : jnp.array([13.0, -8.0, 10.5, 1.0]), \n...               'w0' : jnp.array([1.0, -1.0, 0.0,0.0]), \n...               'b0' : 3.0}\n>>> assert abs(mlp(params, x)-0.07324476)<1e-5\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "2e": {
     "name": "2e",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = jnp.array([[1.0, 2.0, 3.0], [2.0,3.0,4.0],[3.0,4.0,5.0], [5.0,6.0,7.0], [0.0,1.0,2.0]])\n>>> params = {'W' : jnp.array([[1.0, 2.0, 3.0], [-1.0, 2.0, -4.0], [-1.0, -2.0, 5.0], [0.0,0.0,0.0]]), \n...               'b' : jnp.array([13.0, -8.0, 10.5, 1.0]), \n...               'w0' : jnp.array([1.0, -1.0, 0.0,0.0]), \n...               'b0' : 3.0}\n>>> y = jnp.array([0,1,0,0,1])\n>>> assert abs(mlp_loss(params,X,y)-6.3680687)<1e-4\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
